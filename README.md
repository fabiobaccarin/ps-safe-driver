# Porto Seguro's Safe Driver Prediction
Kaggle competition with Porto Seguro's data ([source](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/overview)). Objective: predict the probability of a client filing an insurance claim in the next 12 months.

**Note on reproducibility**. To reproduce the results here, it is ideal that one has [Miniconda3](https://docs.conda.io/en/latest/miniconda.html) installed. Here, I use a Miniconda3 virtual environment, specified in the `env.yml` file, as a working or research environment.

## Contents
- [Project structure](#project-header)
- [Data set](#data-set)
- [Splitting](#splitting)
- [Feature ranking](#feature-ranking)
- [Benchmarking feature groups](#benchmarking-feature-groups)
- [Model comparison](#model-comparison)
- [Conclusion](#conclusion)

## Project structure
```
/data                       <-- Data files
    |-- /interim            <-- Intermediate datasets
    |-- /processed          <-- Preprocessed datasets, used mainly for saving time during fitting
    |-- /raw                <-- Raw data, i.e, as extracted from the source
/make                       <-- Scripts that perform analyses
/models                     <-- Pickled fitted models
/reports                    <-- Reports generated during analyses
    |-- /plots              <-- Plots
    |-- /profiles           <-- HTML files generated by Pandas Profiling
    |-- /tables             <-- Tables
/src                        <-- Python modules and other files related to the construction of the models
    |-- /meta               <-- Metadata for models
.gitignore                  <-- Git ignore file
env.yml                     <-- Conda environment specification
LICENSE                     <-- License file
Makefile                    <-- GNU Make file
README.md                   <-- This file
``` 

## Data set
The data has about 595,212 rows and 57 columns (plus the target). According to the competition description, we have a naming convention as follows. Features's suffixes such as `cat` and `bin` denote categorical or binary features, and features without such suffixes are either numerical or ordinal. Features also have prefixes, such as `car` and `ind`, providing a semantical taxonomy of information. This is not of much use, though, for we cannot know what they mean

## Splitting
I decided to split it into two sets: research, development. Given that I rely heavily on standard errors to judge between models, I don't see the point of making a test set if the train-test dichotomy is already induced by cross-validation. So, I chose to split my data in the following way. Five percent of the data will be to feature research alone (roughly 30k rows), and  the other 95 percent will be to model development

## Feature ranking
I'm a fan of the [Pareto's principle](https://en.wikipedia.org/wiki/Pareto_principle): 80 percent of effects are due to 20 percent of causes. That is, there is only a handful of explanations for every given phenomenon. Today datasets are huge both in the number of rows and in the number of columns. Because of this, the first thing I do when analyzing a new dataset is to answer the following questions:
- Who are the top 5 most important features?
- Are they better than the rest for predicting the target?

I think these questions are of paramount importance. In business, we want things as simple as possible. Simple models are fast to build, run and deploy. They make relatively fast predictions in production environments and consume fewer resources. More important still: simple models are easier to interpret. This is required for mission-critical algorithms or in highly regulated industries. But it is also convenient in the absence of the above conditions because they leverage the domain knowledge of business analysts, product owners and leadership. Understanding your business better boosts the ROI of the model on a massive scale.

So, I want simple things. Simple, not simpler. To figure out which features are the best, I used association measures like [Spearman's rank correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) and [Cramér's V](https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V), along with associated p-values. I chose this approach because it is fast to compute (no need for running thousands of models) and because it is very straightforward to understand. All that is needed is some working knowledge of statistics. The procedure is very simple. For every feature x in the feature matrix X, I check to see if x is a binary feature (i.e., if it has only two distinct values). If that is the case, and because this is a classification problem, I evaluate the feature using Cramér's V. Otherwise, I use the Spearman's rank correlation.

I can only do this because I did some preprocessing of categorical features before evaluating them. For categorical features, I did the following (in order):
1. Group rare labels (labels with less than 5 percent of samples)
2. Encode the resulting labels using target encoding (every label is replaced by the average event
rate in the label)
3. Imputation of missing values by the most frequent encoded label
4. Stabilization of variance by the application of [`QuantileTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html?highlight=quantiletransformer#sklearn.preprocessing.QuantileTransformer), targeting the normal distribution
5. Scaling using [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standardscaler#sklearn.preprocessing.StandardScaler)

The last two are not required, but I will need them to train linear models later on, so I just did this here to unify the preprocessors along the entire analysis. Those steps didn't alter the results.

For numerical features, I did the following (only the first required):
1. Imputation of missing values using the median value
2. Stabilization of variance by the application of [`QuantileTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html?highlight=quantiletransformer#sklearn.preprocessing.QuantileTransformer), targeting the normal distribution
3. Scaling using [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standardscaler#sklearn.preprocessing.StandardScaler)

The results are presented in Figure 1 below. This plot is known as volcano plot ([Kuhn & Johnson (2013)](https://www.amazon.com.br/dp/B00K15TZU0/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1)). On the vertical axis, there are the association measures for every feature (on a 100 point scale), paired with the respective p-value on the horizontal axis (on a negative log 10 scale). These p-values reflect the statistical significance of a hypothesis test where the null hypothesis is that the association measure is equal to zero. The larger the pair (association, significance), the better the feature. The vertical and horizontal black lines indicate the zero values: the farther away from them, the better the feature.

![Figure 1: Ranking of features](reports/plots/01FeatureRanking.png)

You may be asking yourself why I got into the trouble of evaluating both the association measure and its p-value together instead of just one or the other. The reason is that, using just the association measure, we run into the risk of a large sampling error: we will make wrong conclusions if our sample is not representative or if we just got lucky. To avoid that we need to have a measure of how reproducible the result is, and that measure is the p-value (for an intuitive explanation of hypothesis testing, see my article on Towards Data Science [here](https://towardsdatascience.com/statistical-inference-saving-time-and-money-while-making-robust-conclusions-39edc75578f)).

And why not use just the p-value? The reason is that, if my sample is large enough, every association measure, no matter how small, will be deemed statistically significant. Statistical tests are not intended to assess the relevancy of the effect size (i.e., the strength of the effect), but to see if the result is reproducible. Therefore, for a sample of about 30k, pretty much every association measure that is not zero will have some degree of statistical significance. And some of them will be huge (that is why I scaled the p-values on a log 10 scale). As my sample size grows, the statistical significance goes to infinity, even if the effect size is exactly zero (just because the standard error of the statistic goes to zero as the sample size grows).

Therefore, we need both the association measure (effect size) and the p-value (statistical significance) to make informed decisions about picking the best features. The volcano plot provides a graphical way of doing that. We can see that there are features that are better than the rest, but the best of them have just a 5 percent of association with the target. This is bad news: it will be hard to get a pretty good model with such weak features. Also, many features are pretty much the same judging by their association with the target. But we have good news also: the p-values are so big that even if we did some correction to avoid the [multiplicity problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem) like the [Bonferroni correction](https://en.wikipedia.org/wiki/Bonferroni_correction) we would still be able to differentiate between features on any desired level of significance.

But we still don't know who are the best features. For this, I made Table 1 below:

|Feature|Association strength|Statistical significance|Rank|Group|
|:--- |:---: |:---: |:---: |:---: |
|ps_car_13|4.96|16.95|1.00|1.00|
|ps_reg_02|4.51|14.17|2.00|1.00|
|ps_ind_17_bin|4.32|13.27|3.00|1.00|
|ps_car_12|4.06|11.63|4.00|1.00|
|ps_car_04_cat|4.03|11.46|5.00|1.00|
|ps_reg_03|3.99|11.23|6.00|2.00|
|ps_ind_16_bin|3.59|9.47|7.00|2.00|
|ps_ind_07_bin|3.23|7.84|8.00|2.00|
|ps_ind_06_bin|3.15|7.46|9.00|2.00|
|ps_car_08_cat|3.11|7.31|10.00|2.00|
|ps_car_15|3.07|6.91|11.00|3.00|
|ps_ind_15|-3.04|6.79|12.00|3.00|
|ps_car_02_cat|2.98|6.77|13.00|3.00|
|ps_ind_05_cat|2.93|6.57|14.00|3.00|
|ps_ind_10_bin|2.82|6.17|15.00|3.00|
|ps_reg_01|2.81|5.92|16.00|4.00|
|ps_car_03_na_bin|2.52|5.08|17.00|4.00|
|ps_car_09_cat|-2.39|4.42|18.00|4.00|
|ps_car_07_cat|1.83|3.03|19.00|4.00|
|ps_car_11_cat|1.82|3.00|20.00|4.00|
|ps_ind_14|1.64|2.33|21.00|5.00|
|ps_reg_03_na_bin|1.61|2.50|22.00|5.00|
|ps_car_05_na_bin|1.56|2.40|23.00|5.00|
|ps_car_01_cat|1.44|1.89|24.00|5.00|
|ps_car_14|1.34|1.69|25.00|5.00|
|ps_ind_01|1.25|1.51|26.00|6.00|
|ps_ind_12_bin|1.20|1.67|27.00|6.00|
|ps_calc_08|1.05|1.15|28.00|6.00|
|ps_ind_08_bin|1.03|1.38|29.00|6.00|
|ps_car_03_cat|1.00|1.33|30.00|6.00|
|ps_calc_11|0.94|0.98|31.00|7.00|
|ps_ind_02_cat|0.70|0.65|32.00|7.00|
|ps_ind_03|0.67|0.61|33.00|7.00|
|ps_calc_05|0.66|0.59|34.00|7.00|
|ps_calc_15_bin|0.62|0.84|35.00|7.00|
|ps_calc_19_bin|0.57|0.80|36.00|8.00|
|ps_calc_04|0.55|0.46|37.00|8.00|
|ps_ind_09_bin|0.53|0.76|38.00|8.00|
|ps_car_06_cat|0.50|0.41|39.00|8.00|
|ps_calc_03|0.48|0.39|40.00|8.00|
|ps_ind_04_cat|0.45|0.68|41.00|9.00|
|ps_calc_17_bin|0.44|0.68|42.00|9.00|
|ps_ind_13_bin|0.44|0.68|43.00|9.00|
|ps_calc_01|0.44|0.35|44.00|9.00|
|ps_calc_10|-0.42|0.33|45.00|9.00|
|ps_car_11|0.40|0.31|46.00|10.00|
|ps_calc_16_bin|0.40|0.65|47.00|10.00|
|ps_calc_12|-0.25|0.18|48.00|10.00|
|ps_calc_02|0.23|0.16|49.00|10.00|
|ps_calc_06|0.23|0.16|50.00|10.00|
|ps_calc_09|0.21|0.15|51.00|11.00|
|ps_calc_18_bin|0.18|0.53|52.00|11.00|
|ps_calc_13|0.10|0.06|53.00|11.00|
|ps_calc_14|-0.04|0.03|54.00|11.00|
|ps_calc_07|-0.04|0.02|55.00|11.00|
|ps_ind_18_bin|0.00|0.35|58.00|12.00|
|ps_calc_20_bin|0.00|0.14|58.00|12.00|
|ps_ind_11_bin|0.00|0.08|58.00|12.00|
|ps_car_14_na_bin|0.00|0.15|58.00|12.00|
|ps_car_05_cat|0.00|0.45|58.00|12.00|

From this table, we see that the most important feature is `ps_car_13`, with 4.96 percent of association and 16.95 score of statistical significance. It has about 10 percent more association with the target than the second feature, `ps_reg_02` (and about 21 percent more statistical significance). On the top five features, we have associations between 4 and 5 percent. Of those features, three are numerical, one is binary and one is categorical. That is a somewhat diverse group -- not the most diverse, but could be worse. None of them are missing indicators, which is good to know: no problems in gathering information for Porto Seguro.

We already have our star group. What remains to assess before making simple explorative models with different combinations of features is to see if they are redundant. They will be if their [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is very big (I used a threshold of 75 percent). To see if this is the case, I did a correlation matrix (Figure 2 below).

![Figure 2: Correlation matrix of features](reports/plots/02FeaturesCorrMatrix.png)

We can see that there are not many problems with redundancy in this dataset. This is very good news! Eventually, many features will be useful for prediction and we won't have many problems during training and evaluating models. But, to double-check, I did the same plot, but with a filter for showing only the pairs above 75 percent correlation (Figure 3 below).

![Figure 3: Correlation matrix of features - only above 75 percent](reports/plots/03FeaturesCorr75Matrix.png)

We can see that only two features are pathologically correlated (the matrix is symmetrical): `ps_ind_14` and `ps_ind_12_bin` (I looked up their names in the Pandas Profile report on the research feature matrix). None of them are in the top 5 features: the first is in the 21st place and the other in the 27th place. This is very good news, indeed.

## Benchmarking feature groups
To know how the number of features impacts the quality of predictions, I estimated an unregularized logistic regression for each group of 5 features, according to the ranking above. I did this in such a way that the first benchmark will be with only the top 5 features, the second with the top 10 (groups 1 and 2) and so on. Here, I'm interested in knowing if adding progressively irrelevant features brings significant improvements in performance of a simple model. To accomplish this, I estimated 100 logistic regressions for all 12 versions, totaling 1,200 logistic regressions, on repeated stratified cross-validation. Then I calculated the [average precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) for every model, and then calculated its average and the respective standard error at 95 percent of significance. I also measured the time required for each benchmark to have some idea about trade-offs of time in training.

I prefer the average precision to the more popular AUC-ROC because of its nice business interpretation. The average precision tells me the number of true positives I get after controlling for false negatives. So, I can then make some business interpretations of the form "If I label 1,000 clients as positives, I will get X events". From there, I can make some predictions about the ROI of each model. For example, if a model predicts correctly 2X true positives in 1,000 positives then it will reduce the company costs by half if compared to a model predicting X in 1,000. The good thing is that the false negatives are already accounted for in the estimate because of the way Scikit-learn calculates the average precision.

The results are in Table 2 below. We can see pretty interesting results: adding progressively irrelevant features improves the average precision significantly (i.e., results in a difference greater than the estimated margin of error at 95 percent significance). This trend goes on until group 3, totaling 15 features. After that, the improvement is within the margin of error until group 6 (30 features). From there onwards the average precision *declines* until the end.

|Experiment|Mean fit time (ms)|Mean score time (ms)|Mean test average precision (%)|Fit time standard error (ms)|Score time standard error (ms)|Test average precision standard error (%)|Cost-effectiveness ratio|
|:--- |:---: |:---: |:---: |:---: |:---: |:---: |:---: |
|benchmark01 (05 features)|156.53|7.96|5.90|6.26|0.40|0.05|27.88|
|benchmark02 (10 features)|252.76|9.42|6.11|8.16|0.73|0.06|42.91|
|benchmark03 (15 features)|4043.52|8.42|6.63|535.43|0.34|0.06|610.74|
|benchmark04 (20 features)|5326.44|24.05|6.67|651.57|0.79|0.06|802.20|
|benchmark05 (25 features)|17664.87|41.66|6.64|1322.16|1.35|0.06|2668.59|
|benchmark06 (30 features)|20575.47|45.22|6.67|1345.64|0.89|0.06|3092.28|
|benchmark07 (35 features)|23733.66|51.21|6.60|1406.11|1.17|0.06|3604.46|
|benchmark08 (40 features)|16241.47|33.43|6.59|919.39|1.12|0.06|2468.26|
|benchmark09 (45 features)|16823.51|32.01|6.48|938.73|0.57|0.06|2600.33|
|benchmark10 (50 features)|18384.20|36.41|6.45|1020.31|0.63|0.06|2858.08|
|benchmark11 (55 features)|20236.35|41.02|6.37|1112.97|0.71|0.06|3181.53|
|benchmark12 (60 features)|35224.19|45.02|6.26|2693.56|0.66|0.05|5634.41|


We should note that the average precision with all features is higher than the average precision with just the top 5 features, even when considering the margin of error. However, the time needed to fit the model with 60 features is over 200 times the time needed to train with 5 features. Also, the score time is about over 6 times bigger with all 60 features.

On the other hand, by using 15 features, we get a significantly higher average precision and about 9 times more speed in the model. So, 15 features should be our number of features to include in the model: there is no evidence supporting the inclusion of more features than that.

But a higher average precision alone is not enough here. We want the best model for the business, not for our research paper. So we have to consider the costs incurred by each model. To do that, I computed the cost-effectiveness ratio for every model, where the cost is the total fit time and the effectiveness is the average precision. I'm simplifying a little here because I'm assuming that the operational costs of putting a model into production come from processing time only (mainly in the form of fees to cloud providers). Therefore, the cost of running a model into production is proportional to the time it takes to process stuff.

We see that with 15 features we don't have the best ratio, which would be the one for only the top 5 features. This means that the increase in prediction performance is not compensating for the increase in training time or prediction time, even if the increase in performance is statistically significant. For example, fit time increases 61.5 percent when including group 2, but average precision increases by only about 3.6 percent. Moreover, by using 15 features we have a model that is about 54 percent more expensive than the model using only the top 5 features, after normalizing by their respective effectivenesses. Notice that this analysis doesn't depend on cloud computing fees and how much it costs Porto Seguro to respond to a claim. Because the cost and the benefit will be the same for every model (because they will run on the same machine and score the same clients), the relative price of costs in terms of benefits gets out of the equation.

## Model comparison
After deciding on the number of features to use, I fitted different models using grid search cross-validation on the development set. Then, I took the best model under each search and made 50 cross-validations for each of them. My aim here is to assess how well they predict and, more importantly, get a margin of error on those prediction performances. After this, I can reasonably compare different models and see which one is the best.

The analysis is pretty much the same as used in the benchmarking above. The results are shown in Table 3 below. We can see that the Naïve Bayes with Gaussian features is the most cost-effective (under the margin of error, it is a draw with Naïve Bayes with Bernoulli features). This is a very interesting result because the Naïve Bayes is not the best model in average precision alone.

|Model|Mean fit time (ms)|Mean score time (ms)|Mean test average precision (%)|Fit time standard error (ms)|Score time standard error (ms)|Test average precision standard error (%)|Cost-effectiveness ratio|
|:--- |:---: |:---: |:---: |:---: |:---: |:---: |:---: |
|naive_bayes_gaussian|4243.83|590.82|5.33|26.18|6.30|0.02|906.49|
|naive_bayes_bernoulli|4352.76|598.38|5.26|36.87|11.34|0.02|940.40|
|logistic_regression|8819.66|463.18|5.40|56.08|5.74|0.02|1720.43|
|lasso|9396.20|493.42|5.40|104.76|8.26|0.02|1832.85|
|elastic_net|9428.55|493.78|5.40|129.02|9.70|0.02|1838.91|
|ridge|9528.90|505.77|5.40|125.03|10.29|0.02|1859.75|
|decision_tree|10178.74|513.50|3.68|134.34|6.32|0.00|2905.86|
|light_gbm|35080.39|1787.03|5.43|138.69|17.90|0.02|6795.69|

Look at logistic regression, for example. It has 5.4 percent average precision, losing only to Light GBM by just 0.03 percentage points (which means only 3 more correctly predicted positives in 10,000). The logistic regression is also about 22 percent faster to score than the most cost-effective model. But score time alone is not the only cost we incur to provide this model. If we look at fit times, the logistic regression is two times slower than the best model, which results in it being the third in the cost-effectiveness ranking. If we assume a discount rate of exactly zero and that the models will be run monthly, the logistic regression would need 33 months to start being cheaper than gaussian Naïve Bayes. By then we would probably have retrained the model because of data drift, so logistic regression would never catch up. Note: we assumed a zero discount rate; if we assumed a positive discount rate, it will take even longer. This analysis depends on the time we want to have the model on production before retraining and on its frequency of execution. If the job ran daily, we would need just over a month to recover the investment in training, and in the "long run" the logistic regression would be the most cost-effective model.

## Conclusion
With these results, I would implement the gaussian Naïve Bayes model with only the top 5 features. This is not the best model judging by average precision alone, but is the best one from a business perspective. That is, it is the one that gives the best return for the costs of delivering it.

We saw that adding more features doesn't cover the expenses incurred for doing so, and we would lose money if we added more features. Also, the gaussian Naïve Bayes loses to logistic regression by only 0.07 percentage point in average precision, which amounts to only 7 correctly predicted positives in 10,000. The best model overall in average precision, the Light GBM, manages to get only 10 in 10,000 compared to gaussian Naïve Bayes. Given the amount of time (and therefore, money) needed to produce these other two models and run them in production, these improvements in predictive performance won't compensate for the costs.

When considering the expected lifetime of the models and the frequency of their application, the conclusion might change in favor of logistic regression. This is because this model is faster to score than the gaussian Naïve Bayes and therefore should compensate for the difference in training costs if given enough time. Without considering the time-value of money, the estimated number of periods for logistic regression to start being better is 33. If we expect the models to live longer than that, we should go with logistic regression. If not, we should stick with gaussian Naïve Bayes. This is because, if the models live less than 33 periods, logistic regression would never catch up in operational costs.